---
layout: post
title: Markov Chain Monte Carlo
published: true
---

We want to sample from some distribution $p(z)$ in order to be able to calculate empirical expectations of functions $f(z)$ like

$$
E_{p(z)}\[ f(z) \] \approx \frac{1}{T} \sum\limits_{t=1}^T f(z^{(t)}
$$


# Rejection sampling

Imagine plotting the density function $f\(x\)$ of a random variable $X$ onto a large rectangular board and throwing darts at it. Assume that the darts are uniformly distributed around the board. Now remove all of the darts that are outside the area under the curve. The remaining darts will be distributed uniformly within the area under the curve, and the $x$-positions of these darts will be distributed according to the random variable's density. This is because there is the most room for the darts to land where the curve is highest and thus the probability density is greatest.

<p>
<img style="display:block; margin:auto;" src="{{ "/images/rejectionsamplingcriterion.png" | absolute_url }}">
</p>
You can be more efficient by using an envelope density rather than just a rectangle. Use a density you can sample from called a "proposal density" $g\( v \)$ for a random variable $V$. In fact you can even scale the density of $V$ by $M$: $Mg\(v\)$. Then to sample from $X$ you proceed by sampling $v$ from the proposal density (this gives you the position on the $x$-axis) and sampling $U$ **uniformly** $u \in \[0, Mg\(v\)\]$. Finally accept the sample if $u < f\(v\)$ and reject otherwise. In toto this produces pairs $\(v, u\)$ uniformly distributed over the subgraph of $f\(x\)$ with $P\(v\leq x| \mbox{accept}\) = P_{X}\(x\)$:


$$
\begin{eqnarray}
P \( V \leq x | \mbox{accept} \) &=& \frac{ P\( V\leq x, \mbox{accept} \) }{ P\( \mbox{accept} \) } \nonumber \\

&=& \frac{ P\( V\leq x, U \leq f(V) \) }{ P\( U \leq f(V) \) } \nonumber \\
&=& \frac{ \int\limits_{-\infty}^x \int\limits_0^{f(v)} p_{U,V} (u,v) dudv    }{ \int\limits_{-\infty}^{\infty} \int\limits_0^{f(v)} p_{U,V} (u,v) dudv           } \nonumber \\
\end{eqnarray}
$$

Note that the density of $U$ is $1/Mg(V)$ and that $U,V$ are independent and hence $p_{U, V} = g(v)/Mg(v) = 1/M$. Hence

$$
P \( V \leq x | \mbox{accept} \) = \frac{ \int\limits_{-\infty}^x  f(v) dv    }{ \int\limits_{-\infty}^{\infty} f(v) dv           }  = P_X(x)
$$

Note that because $U$ is uniform this procedure is equivalent to drawing $v$ from the proposal density and then accepting it with probability $f(v)/Mg(v)$ and rejecting with probability $1- f(v)/Mg(v)$ (i.e. flipping a biased coin). **This ratio-test-accept-reject is an important sampling paradigm**.

Note also that the probability of accepting is 

$$\begin{eqnarray}
P\(U\leq \frac{f(V) } { Mg(V) } \) &=& E \[  P\(U\leq \frac{f(V) } { Mg(V) } \)  \Bigg|V \] \nonumber\\
&=& E \[\frac{f(V) } { Mg(V) } \Bigg| V \] {\text{ because }}P(U\leq u)=u,{\text{when }}U{\text{ is uniform on }}(0,1) \nonumber\\
&=&  \int\limits _{v:g(v)>0}\frac{f(v) } { Mg(v) } g(v)dv         \nonumber\\
&=& \frac{1}{M} \int\limits _{v:g(v)>0}f(v)dv         \nonumber\\
&=& \frac{1}{M}

\end{eqnarray}$$

and hence expected acceptance time is related to the geometric distribution i.e. $M$ iterations hence you want to choose $M$ as small as possible (while still satisfying $Mg(v) \geq f(v)$ for all $v$).

If normalizing constants $Z_f, Z_g$ for both distributions $f,g$ are unknown (as in Bayesian inference) rejection sampling still works: just define $M' = Z_f/MZ_g$ and then $f/g \leq M \iff f'/g' \leq M'$. Practically this means you can ignore normalizing constants from the start.

## Application to Bayesian inference

Suppose we want to draw from the posterior $p(\theta \| x\) = p\(x\| \theta  \) p\(\theta)/p\(x\)$. We can use rejection sampling with $f(\theta) = p\(x\| \theta  \) p\(\theta)$ as the target distribution and $g(\theta)=p(\theta)$ as the proposal distribution with $M= p(x\| \hat{\theta})$ where $\hat{\theta}$ is the MLE. Then we draw from the prioer and accept points with probability 

$$
\frac{f(\theta)}{Mg(\theta)} = \frac{p(x|\theta)}{p(x|\hat{\theta})}
$$

## Adaptive rejection sampling

There's an automatic way to come up with a tight envelope density $g(x)$ to any log-concave <sup>[1](#myfootnote1)</sup> density $f(x)$: upper-bound the log-density with a piecewise linear function where initial points of tangency are chosen on a regular grid and the slopes are the gradients of $\log\[f(x)\]$ at those grid points. Then $g(x)$ is the piecewise exponential

$$
g(x) = M_i \lambda_i e^{-\lambda_i (x-x_i)}, x_{i-1} < x \leq x_i
$$

where $x_i$ are the gridpoints, $M_i$ are determined as usual, and $\lambda_i$ fit the line in log-density space. Sampling from exponential distributions is easy. Further there is refinement to the envelope: if a sample $x$ is rejected then, since we've evaluated $f(x)$, we can add another gridpoint and thereby tighten the envelope.

# Interlude


For our purposes, the key idea of MCMC is to think of $z$’s as points in a state space and find ways to “walk around” the space — going from $z^{(0)}$ to $z^{(1)}$ to $z^{(2)}$ and so forth — so
that the likelihood of visiting any point $z$ is proportional to $p(z)$. A walk of this kind can be characterized abstractly as follows:

```
$z^{(0)} \gets$ random()
for $t = 1$ to $T$:
    $z^{(t+1)} \gets g\( z^{(t)} \)$
```
Here $g$ is a function that makes a probabilistic choice about what state to go to next according to an
explicit or implicit transition probability 

$$

P_{trans}\(z^{(t+1)}|z^{(0)}, z^{(1)}, ... , z^{(t)}\) 

$$

One way to implement this plan (where the state space walk spends most of its time in areas of high $p(z)$) is to construct a Markov chain since the Markov chain will naturally spend most of its time in such areas (if the chain has a stationary distribution and it's aperiodic).

Therefore the heart of Markov Chain Monte Carlo methods is designing a chain so that the probability of visiting a state $z$
will turn out to be $p(z)$, as desired. This can be accomplished by guaranteeing that the chain, as defined by
the transition probabilities $P_{\mbox{trans}}$, meets certain conditions. Gibbs sampling is one algorithm that meets
those conditions.

# Gibbs sampling

Super anti-climactic (at least until you get to the proof). Gibbs sampling is applicable in situations where $Z$ is a vector quantity with $k$ dimensions. The idea is rather than picking the next state all at once, you make a separate probabilistic choice for each of the $k$ dimensions where each choice depends on the previous $k-1$ choices. That is, the walk through the space proceeds as follows

```
$z^{(0)} \gets \langle z_1^{(0)}, \dots, z_k^{(0)}  \rangle$
for $t = 1$ to $T$:
    $z_i^{(t+1)} \gets g\( z^{(t)} \)$
    for $i=1$ to $k$:
        $z^{(t+1)} \sim P\( Z_i\bigg| z_1^{(t+1)}, \dots, z_{i-1}^{(t+1)}, z_{i+1}^{(t)}, \dots, z_k^{(t)} \)$
```

How you actually draw 
$z_i^{(t+1)} \sim P\( Z_i\bigg| z_1^{(t+1)}, \dots, z_{i-1}^{(t+1)}, z_{i+1}^{(t)}, \dots, z_k^{(t)} \)$ depends on your model but that this process actually encodes a reversible Markov chain walk can be proved as follows: define $g$ as any distribution for $z$ and $z' \sim_j z''$ if $z_i' = z_i''$ for all $i \neq j$ (i.e. $z'$ and $z''$ are the same everywhere except possibly in the $j$th position) denote the probability of a jump from $z' $ to $z'' $ as $p_{z'z''}$. Then the transition probabilities are 

$$
p_{z'z''}={\begin{cases}{\frac  {1}{k}}{\frac  {g(z'')}{\sum _{y\in \Theta :y\sim _{j}z'}g(y)}}&z'\sim _{j}z''\\0&{\text{otherwise}}\end{cases}}
$$

i.e. the probability of transitioning from $z'$ to $z''$ is the probability of $z''$ divided by the sum of all of the probabilities of possible transitions (those that differ from $z'$ at only position $j$). The $1/d$ is to account for overcounting?

Clearly $g(z')p_{z'z''} = g(z'' )p_{z'' z'}$ since  $z' \sim_j z''$ is an equivalence relation hence the detailed balance equations are satisfied which implies the chain is reversible and it has invariant distribution $g$.

In practice, the index is not chosen at random, and the chain cycles through the suffixes in order. In general this gives a non-stationary Markov process, but each individual step will still be reversible, and the overall process will still have the desired stationary distribution (as long as the chain can access all states under the fixed ordering).

<a name="myfootnote1">1</a>: For strictly positive $f$ $ \log\[f(\theta x+(1-\theta )y)\]\geq \theta \log\[f(x)]+ \(1-\theta\) \log\[f(y)\]$


