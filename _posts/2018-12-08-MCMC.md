---
layout: post
title: Markov Chain Monte Carlo
published: true
---

We want to sample from some distribution $p(z)$ in order to be able to calculate empirical expectations of functions $f(z)$ like

$$
E_{p(z)}\[ f(z) \] \approx \frac{1}{T} \sum\limits_{t=1}^T f(z^{(t)}
$$

# Sampling
## Rejection sampling

Imagine plotting the density function $f\(x\)$ of a random variable $X$ onto a large rectangular board and throwing darts at it. Assume that the darts are uniformly distributed around the board. Now remove all of the darts that are outside the area under the curve. The remaining darts will be distributed uniformly within the area under the curve, and the $x$-positions of these darts will be distributed according to the random variable's density. This is because there is the most room for the darts to land where the curve is highest and thus the probability density is greatest.

<p>
<img style="display:block; margin:auto;" src="{{ "/images/rejectionsamplingcriterion.png" | absolute_url }}">
</p>
You can be more efficient by using an envelope density rather than just a rectangle. Use a density you can sample from called a "proposal density" $g\( v \)$ for a random variable $V$. In fact you can even scale the density of $V$ by $M$: $Mg\(v\)$. Then to sample from $X$ you proceed by sampling $v$ from the proposal density (this gives you the position on the $x$-axis) and sampling $U$ **uniformly** $u \in \[0, Mg\(v\)\]$. Finally accept the sample if $u < f\(v\)$ and reject otherwise. In toto this produces pairs $\(v, u\)$ uniformly distributed over the subgraph of $f\(x\)$ with $P\(v\leq x| \mbox{accept}\) = P_{X}\(x\)$:


$$
\begin{eqnarray}
P \( V \leq x | \mbox{accept} \) &=& \frac{ P\( V\leq x, \mbox{accept} \) }{ P\( \mbox{accept} \) } \nonumber \\

&=& \frac{ P\( V\leq x, U \leq f(V) \) }{ P\( U \leq f(V) \) } \nonumber \\
&=& \frac{ \int\limits_{-\infty}^x \int\limits_0^{f(v)} p_{U,V} (u,v) dudv    }{ \int\limits_{-\infty}^{\infty} \int\limits_0^{f(v)} p_{U,V} (u,v) dudv           } \nonumber \\
\end{eqnarray}
$$

Note that the density of $U$ is $1/Mg(V)$ and that $U,V$ are independent and hence $p_{U, V} = g(v)/Mg(v) = 1/M$. Hence

$$
P \( V \leq x | \mbox{accept} \) = \frac{ \int\limits_{-\infty}^x  f(v) dv    }{ \int\limits_{-\infty}^{\infty} f(v) dv           }  = P_X(x)
$$

Note that because $U$ is uniform this procedure is equivalent to drawing $v$ from the proposal density and then accepting it with probability $f(v)/Mg(v)$ and rejecting with probability $1- f(v)/Mg(v)$ (i.e. flipping a biased coin). **This ratio-test-accept-reject is an important sampling paradigm**.

Note also that the probability of accepting is 

$$
\begin{eqnarray}
P\(U\leq \frac{f(V) } { Mg(V) } \) &=& E \[  P\(U\leq \frac{f(V) } { Mg(V) } \)  \Bigg|V \] \nonumber\\
&=& E \[\frac{f(V) } { Mg(V) } \Bigg| V \] {\text{ because }}P(U\leq u)=u,{\text{when }}U{\text{ is uniform on }}(0,1) \nonumber\\
&=&  \int\limits _{v:g(v)>0}\frac{f(v) } { Mg(v) } g(v)dv         \nonumber\\
&=& \frac{1}{M} \int\limits _{v:g(v)>0}f(v)dv         \nonumber\\
&=& \frac{1}{M}

\end{eqnarray}
$$

and hence expected acceptance time is related to the geometric distribution i.e. $M$ iterations hence you want to choose $M$ as small as possible (while still satisfying $Mg(v) \geq f(v)$ for all $v$).

If normalizing constants $Z_f, Z_g$ for both distributions $f,g$ are unknown (as in Bayesian inference) rejection sampling still works: just define $M' = Z_f/MZ_g$ and then $f/g \leq M \iff f'/g' \leq M'$. Practically this means you can ignore normalizing constants from the start.

## Application to Bayesian inference

Suppose we want to draw from the posterior $p\(\theta \| x\) = p\(x\| \theta  \) p\(\theta\)/p\(x\)$. We can use rejection sampling with $f(\theta) = p\(x\| \theta  \) p\(\theta\)$ as the target distribution and $g(\theta)=p(\theta)$ as the proposal distribution with $M= p(x\| \hat{\theta})$ where $\hat{\theta}$ is the MLE. Then we draw from the prior and accept points with probability 

$$
\frac{f(\theta)}{Mg(\theta)} = \frac{p(x|\theta)}{p(x|\hat{\theta})}
$$

## Adaptive rejection sampling

There's an automatic way to come up with a tight envelope density $g(x)$ to any log-concave <sup>[1](#myfootnote1)</sup> density $f(x)$: upper-bound the log-density with a piecewise linear function where initial points of tangency are chosen on a regular grid and the slopes are the gradients of $\log\[f(x)\]$ at those grid points. Then $g(x)$ is the piecewise exponential

$$
g(x) = M_i \lambda_i e^{-\lambda_i (x-x_i)}, x_{i-1} < x \leq x_i
$$

where $x_i$ are the gridpoints, $M_i$ are determined as usual, and $\lambda_i$ fit the line in log-density space. Sampling from exponential distributions is easy. Further there is refinement to the envelope: if a sample $x$ is rejected then, since we've evaluated $f(x)$, we can add another gridpoint and thereby tighten the envelope.

# Interlude

For our purposes, the key idea of MCMC is to think of $z$’s as points in a state space and to find ways to “walk around” the space — going from $z^{(0)}$ to $z^{(1)}$ to $z^{(2)}$ and so forth — so
that the likelihood of visiting any point $z$ is proportional to $p(z)$. A walk of this kind can be characterized abstractly as follows:

```
$z^{(0)} \gets$ random()
for $t = 1$ to $T$:
    $z^{(t+1)} \gets g\( z^{(t)} \)$
```
Here $g$ is a function that makes a probabilistic choice about what state to go to next according to an
explicit or implicit transition probability 
$$
P_{trans}\(z^{(t+1)}|z^{(0)}, z^{(1)}, ... , z^{(t)}\)
$$

One way to implement this plan (where the state space walk spends most of its time in areas of high $p(z)$) is to construct a Markov chain since the Markov chain will naturally spend most of its time in such areas (if the chain has a stationary distribution and it's aperiodic).

Therefore the heart of Markov Chain Monte Carlo methods is designing a chain so that the probability of visiting a state $z$ will turn out to be $p(z)$, as desired. This can be accomplished by guaranteeing that the chain, as defined by the transition probabilities $P_{\mbox{trans}}$, meets certain conditions. Gibbs sampling is one algorithm that meets those conditions.



# Markov Chains

## Basic Definition

A **Markov chain** is a [stochastic model](https://en.wikipedia.org/wiki/Stochastic_model) describing a [sequence](https://en.wikipedia.org/wiki/Sequence) of possible events in which the probability of each event depends only on the state attained in the previous event, i.e. in the discrete case

$$
P(X_{n+1}=x\mid X_{1}=x_{1},X_{2}=x_{2},\ldots ,X_{n}=x_{n})=P(X_{n+1}=x\mid X_{n}=x_{n})
$$

Markov chains are often described by a sequence of directed graphs, where the edges of graph n are labeled by the probabilities of going from one state at time n to the other states at time $n + 1$, $ P(X_{n+1}=x\mid X_{n}=x_{n})$. The same information is represented by the transition matrix from time n to time n + 1. **Time-homogeneous Markov chains** (or **stationary Markov chains**) are processes where

$$
{\displaystyle P(X_{n+1}=x\mid X_{n}=y)=P(X_{n}=x\mid X_{n-1}=y)}
$$

i.e. for all $n$ the probability of the transition is independent of $n$.

## Example

Here is an example modeling a hypothetical stock market

<p>
<img style="display:block; margin:auto;" src="{{ "/images/finance_Markov_chain_example_state_space.svg" | absolute_url }}">
</p>

Labelling the state space {1 = bull, 2 = bear, 3 = stagnant} the transition matrix for this example is

$$
P={\begin{bmatrix}0.9&0.075&0.025\\0.15&0.8&0.05\\0.25&0.25&0.5\end{bmatrix}}.
$$

Using the transition matrix it is possible to calculate, for example, the long-term fraction of weeks during which the market is stagnant, or the average number of weeks it will take to go from a stagnant to a bull market. Using the transition probabilities, the steady-state probabilities indicate that 62.5% of weeks will be in a bull market, 31.25% of weeks will be in a bear market and 6.25% of weeks will be stagnant, since:

$$
{\displaystyle \lim _{N\to \infty }\,P^{N}={\begin{bmatrix}0.625&0.3125&0.0625\\0.625&0.3125&0.0625\\0.625&0.3125&0.0625\end{bmatrix}}}
$$

## Some definitions

- A Markov chain is said to be **irreducible** if it is possible to get to any state from any state i.e. the underlying directed graph is strongly connected.

- A state *i* is said to be **transient** if, given that we start in state *i*, there is a non-zero probability that we will never return to *i*. State *i* is **recurrent** (or **persistent**) if it is not transient. Recurrent states are guaranteed (with probability 1) to have a finite hitting time. Even if the hitting time is finite with probability $1$, it need not have a finite [expectation](https://en.wikipedia.org/wiki/Expected_value). The **mean recurrence time** at state *i* is the expected return time $M_i$:

  $$
  {\displaystyle M_{i}=E[T_{i}]=\sum _{n=1}^{\infty }n\cdot f_{ii}^{(n)}.}
  $$

  State *i* is **positive recurrent** (or **non-null persistent**) if *Mi* is finite; otherwise, state *i* is **null recurrent** (or **null persistent**).

- A state *i* has **period** *k* if any return to state *i* must occur in multiples of *k* time steps. Formally, the [period](https://en.wikipedia.org/wiki/Periodic_function) of a state is defined as

  $$
  k=\gcd\{n>0:P(X_{n}=i\mid X_{0}=i)>0\}
  $$

  provided that this set is not empty. Otherwise the period is not defined. If *k* = 1, then the state is said to be **aperiodic**. Otherwise (*k* > 1), the state is said to be **periodic with period k**. A Markov chain is aperiodic if every state is aperiodic. An irreducible Markov chain only needs one aperiodic state to imply all states are aperiodic.

- A state *i* is said to be **ergodic** if it is aperiodic and positive recurrent. In other words, a state *i* is ergodic if it is recurrent, has a period of *1*, and has finite mean recurrence time. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic.

- If the Markov chain is a [time-homogeneous Markov chain](https://en.wikipedia.org/w/index.php?title=Time-homogeneous_Markov_chain&action=edit&redlink=1), so that the process is described by a single, [time-independent matrix](https://en.wikipedia.org/w/index.php?title=Time-independent_matrix&action=edit&redlink=1) , then the vector ${\displaystyle {\boldsymbol {\pi }}}$ is called a **stationary distribution** (or **invariant measure**) if ${\displaystyle \forall j\in S}$ it satisfies

  $$
  \begin{eqnarray}
  0\leq \pi _{j}\leq 1 \nonumber \\
   \sum _{j\in S}\pi _{j}=1 \nonumber \\
   \pi _{j}=\sum _{i\in S}\pi _{i}p_{ij} \nonumber\\
  \end{eqnarray}
  $$

- A Markov chain is said to be **reversible** if there is a probability distribution **π** over its states such that

  $$
  \pi _{i}P(X_{n+1}=j\mid X_{n}=i)=\pi _{j}P(X_{n+1}=i\mid X_{n}=j)
  $$

  for all times n and all states i and j. This condition is known as the detailed balance condition (some books call it the local balance equation). 





## Theorems

### Equilibrium distributions

An irreducible chain has a positive stationary distribution (a stationary distribution such that ${\displaystyle \forall i,\pi _{i}>0}$ if and only if all of its states are positive recurrent. In that case, $\pi$ is unique and is related to the expected return time:

$$
\pi _{j}={\frac {C}{M_{j}}}
$$

where $C$ is the normalizing constant. Further, if the positive recurrent chain is both irreducible and aperiodic, it is said to have a *limiting* distribution; for any *i* and *j*

$$
{\displaystyle \lim _{n\rightarrow \infty }p_{ij}^{(n)}={\frac {C}{M_{j}}}.}
$$

There is no assumption on the starting distribution; the chain converges to the stationary distribution regardless of where it begins. Such $\pi$ is called the **equilibrium distribution** of the chain.

### Reversible Markov Chain

Considering a fixed arbitrary time *n* and using the shorthand

$$
{\displaystyle p_{ij}=P(X_{n+1}=j\mid X_{n}=i)\,,}
$$

Then the detailed balance equation can be written more compactly as 

$$
\pi _{i}p_{ij}=\pi _{j}p_{ji}
$$

Note that 

$$
\sum _{i}\pi _{i}p_{ij}=\sum _{i}\pi _{j}p_{ji}=\pi _{j}\sum _{i}p_{ji}=\pi _{j}
$$

As $n$ was arbitrary, this reasoning holds for any $n$, and therefore for reversible Markov chains $\pi$ is always a steady-state distribution for every $n$.

Reversible Markov chains are common in MCMC because the detailed balance equation for a desired distribution $\pi$ necessarily implies that the Markov chain has been constructed so that $\pi$ is a steady-state distribution. 

# Gibbs sampling

Super anti-climactic (at least until you get to the proof). Gibbs sampling is applicable in situations where $Z$ is a vector quantity with $k$ dimensions. The idea is rather than picking the next state all at once, you make a separate probabilistic choice for each of the $k$ dimensions where each choice depends on the previous $k-1$ choices. That is, the walk through the space proceeds as follows

```
$z^{(0)} \gets \langle z_1^{(0)}, \dots, z_k^{(0)}  \rangle$
for $t = 1$ to $T$:
    $z_i^{(t+1)} \gets g\( z^{(t)} \)$
    for $i=1$ to $k$:
        $z^{(t+1)} \sim P\( Z_i\bigg| z_1^{(t+1)}, \dots, z_{i-1}^{(t+1)}, z_{i+1}^{(t)}, \dots, z_k^{(t)} \)$
```

How you actually draw 
$z_i^{(t+1)} \sim P\( Z_i\bigg| z_1^{(t+1)}, \dots, z_{i-1}^{(t+1)}, z_{i+1}^{(t)}, \dots, z_k^{(t)} \)$ depends on your model but that this process actually encodes a reversible Markov chain walk can be proved as follows: define $g$ as any distribution for $z$ and $z' \sim_j z''$ if $z_i' = z_i''$ for all $i \neq j$ (i.e. $z'$ and $z''$ are the same everywhere except possibly in the $j$th position) denote the probability of a jump from $z' $ to $z'' $ as $p_{z'z''}$. Then the transition probabilities are 
$$
p_{z'z''}={\begin{cases}{\frac  {1}{k}}{\frac  {g(z'')}{\sum _{y\in \Theta :y\sim _{j}z'}g(y)}}&z'\sim _{j}z''\\0&{\text{otherwise}}\end{cases}}
$$

i.e. the probability of transitioning from $z'$ to $z''$ is the probability of $z''$ divided by the sum of all of the probabilities of possible transitions (those that differ from $z'$ at only position $j$). The $1/d$ is to account for overcounting?

Clearly $g(z')p_{z'z''} = g(z'' )p_{z'' z'}$ since  $z' \sim_j z''$ is an equivalence relation hence the detailed balance equations are satisfied which implies the chain is reversible and it has invariant distribution $g$.

In practice, the index is not chosen at random, and the chain cycles through the suffixes in order. In general this gives a non-stationary Markov process, but each individual step will still be reversible, and the overall process will still have the desired stationary distribution (as long as the chain can access all states under the fixed ordering).

<a name="myfootnote1">1</a>: For strictly positive $f$ : $ \log\[f(\theta x+(1-\theta )y)\]\geq \theta \log\[f(x)\]+ \(1-\theta\) \log\[f(y)\]$


