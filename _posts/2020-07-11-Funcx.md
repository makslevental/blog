---
layout: post
title: High Performance Function(s) as a Service
published: false
---

# Motivation

Full disclosure: I’m a PhD student in the research group that develops *func*X.

Are you a scientist that suffers from bursitis[^1]? 
Do you loathe dealing with runtimes and infrastructure? 
Is your favorite calculus the $\lambda$-calculus? 
Do you have commitment issues with respect to your cloud provider?
Well do I have an offering for you; presenting a High Performance Function(s) as a Service (HPFaaS) Python framework called [*func*X](https://funcx.org/).

<p align="center">
  <img src="/images/inflatable.gif"/>
</p>

In all seriousness though; HPFaaS are a software development paradigm where the fundamental unit of computation is the function and everything else is abstracted away.
Availing oneself of these abstractions enables one to benefit from data-compute locality[^2] and distribution to heterogeneous resources (such as GPUs, FPGAs, and ASICs).
Another name for this kind of software is "serverless computing"; in the context of the kinds of work loads that scientists typically have, we call this "serverless supercomputing".

Some example projects that use *func*X are:

* Synchrotron Serial Crystallography is a method for imaging small crystal samples 1–2 orders of magnitude faster than other methods; using *func*X SSX researchers were able to discover a [new structure related to COVID](https://www.rcsb.org/structure/6XKM)
* [DLHub](https://www.dlhub.org/) uses *func*X to support the publication and servingof ML models for on-demand inference for scientific use cases
* Large distributed file systems produce new metadata at high rates; [Xtract](https://dl.acm.org/doi/abs/10.1145/3366623.3368140) uses *func*X to extract metadata colocal with the data rather than by aggregating centrally
* [Real-time High-energy Physics](https://projects.iq.harvard.edu/atrisovic/publications/real-time-hep-analysis-funcx-high-performance-platform-function-service) analysis using Coffea and *func*X can accelerate studies of decays such as H$\rightarrow$bb.
<!-- * *Quantitative neurocartography* and *connectomics* involve mapping connections in the brain; [auTomo](https://automo.readthedocs.io/en/latest/) uses *func*X as the backing for an automated pipeline that performs quality control on raw brain image data (itself used to calibrate instruments) -->


# *func*X

*func*X works by deploying the *func*X manager on an arbitrary computer, registering a *func*X function with a centralized registry, and then calling the function using either the Python SDK or a REST API. 
For purposes of brevity we defer discussion of [establishing a *func*X endpoint](https://funcx.readthedocs.io/en/latest/endpoints.html) until the next section and make use of the [tutorial endpoint](https://hub.gke.mybinder.org/user/funcx-faas-funcx-5z0wfg6s/notebooks/examples/Tutorial.ipynb#Running-a-function).

To declare a *func*X function we just define a conventional Python function

```python
def funcx_sum(items):
    return sum(items)
```

To register the function with the centralized *func*X function registry service we simply call `register_function`:

```python
from funcx.sdk.client import FuncXClient
fxc = FuncXClient()
func_uuid = fxc.register_function(
    funcx_sum,
    description="A summation function"
)
```

The `func_uuid` is then used to call the function on an endpoint; using the tutorial `endpoint_uuid`:

```python
endpoint_uuid = '4b116d3c-1703-4f8f-9f6f-39921e5864df'
items = [1, 2, 3, 4, 5]
res = fxc.run(
    items, 
    endpoint_id=endpoint_uuid, 
    function_id=func_uuid
)
fxc.get_result(res)
>>> 15
```

And that's all there is to it!
The only caveat (owing to how *func*X serializes functions) is that all libraries/packages used in the function need to be imported within the body of the function, e.g. 

```python
def funcx_sum_2(items):
    from numpy import sum
    return sum(items)
```

# Deploying a *func*X endpoint



# Architecture and Implementation

*func*X consists of endpoints and a registry that publishes endpoints and registered functions:

<p align="center">
  <img src="/images/funcx_arch.png" width="500"/>
</p>

Each endpoint runs a manager daemon that manages executors that themselves orchestrate a pool of *workers* that run *func*X functions within containers[^3]:

<p align="center">
  <img src="/images/funcx_manager.png" width="500"/>
</p>

The manager also implements fault tolerance facilities using a watch dog process and heartbeats from the executors.

Communication between the *func*X service, the managers, and the executors is all over [ZeroMQ](https://zeromq.org/).
For all of the misers[^4] in the audience, *func*X implements all of the standard optimization strategies to make execution more efficient with respect to latency and compute (memoization, container warming, request batching).
For the paranoiacs[^4] in the audience, *func*X authenticates and authorizes registering and calling functions using [Globus Auth](https://docs.globus.org/api/auth/specification/) and sandboxes functions using containerization and file system namespacing therein.
More details (along with performance metrics and comparisons with commercial competitors) are available in the [*func*X paper](https://arxiv.org/abs/1908.04907).

# Conclusion

*func*X is for scientists that have compute needs that fluctuate dramatically in time and resource requirements.
The project is open source (available on [GitHub](https://github.com/funcx-faas/funcX)) and provides a [binder instance](https://mybinder.org/v2/gh/funcx-faas/funcx/master?filepath=examples%2FTutorial.ipynb) that you can immediately experiment with.
If you have any questions or you’re interested in contributing feel free to reach out to the project or myself directly!

# Foonotes

[^1]: Not the mallady but bursty compute loads.
[^2]: Where the compute is co-located with the data for reduced I/O latency.
[^3]: Various containerization systems on different platforms (Docker, Kubernetes, [Singularity](https://sylabs.io/), [Shifter](https://github.com/NERSC/shifter)).
[^4]: Just kidding!
[^5]: Paper forthcoming!
