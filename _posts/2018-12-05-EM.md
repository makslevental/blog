---
layout: post
title: Expectation Maximization
published: true
---

The EM algorithm proceeds by alternatingly taking the expectation of a function $Q$ related to the log-likelihood and maximizing that same function in order to get converging estimates
 $\theta^{t}$ for some parameter $\theta$. The inspiration is 
computing the maximum likelihood estimate for some parameter $\theta$ based on some observed data $\(x_1, x_2, ..., x_i, ..., x_N\)$ and some latent variables $\(z_1, z_2, ..., z_i, ..., z_N\)$.

# The auxiliary function $Q\(\theta, \theta^{t-1}\)$

To compute the MLE you maximize the log-likelihood 


$$
l \left( \theta \right) = \sum_{i=1}^N \log \left( p\left(x_i | \theta \right)\right) = \sum_{i=1}^N \log \sum_{z_i}p\( x_i, z_i | \theta \)
$$

In general you can't do this because you can't "distribute" log across the sum over $z_i$. Instead maximize the "complete data log-likelihood":

$$
l_c \left( \theta \right) = \sum_{i=1}^N \log \big[ p\( x_i, z_i | \theta \) \big] = \sum_{i=1}^N \log \big[ p\( z_i | \theta \) p\( x_i | z_i , \theta \) \big]
$$

Too bad you can't compute this either since the $z_i$ are unobserved. Fortunately you can condition  $p\(z_i | x_i, \theta^{t-1}\)$ on the data $x_i$ and previous estimate of the parameters
$\theta^{t-1}$ and then take expectations. This serves the purpose of "filling in" the missing $z_i$ values and producing a function that's only a function of $\theta$
which you can then maximize. We want to first compute the "auxiliary function"

$$
\begin{eqnarray} 
Q\( \theta, \theta^{t-1} \) &=& E\big[l_c \left( \theta \right) | x_i, \theta^{t-1} \big]       \nonumber \\
  &=& E\Bigg[ \sum_{i=1}^N \log \big[ p\( z_i | \theta \) p\( x_i | z_i , \theta \) \big] \Bigg| x_i, \theta^{t-1} \Bigg] \nonumber \\
  &=&  \sum_{i=1}^N E\bigg[\log \big[ p\( z_i | \theta \) p\( x_i | z_i , \theta \) \big] \bigg| x_i, \theta^{t-1} \bigg]    \nonumber
\end{eqnarray} 
$$

 Keep in mind what $E\[ g\(z_i\) \| x_i, \theta^{t-1} \]$ means here:

$$
E\big[ g\(z_i\) | x_i, \theta^{t-1} \big]  = \int g\(z_i\) p\(z_i | x_i, \theta^{t-1}\) dz_i
$$

or in the discrete case

$$
E\big[ g\( z_i \) | x_i, \theta^{t-1} \big]  = \sum_{k=1}^K g\( z_i \) p\(z_i = k | x_i, \theta^{t-1}\)
$$

i.e. an expectation over the conditional distribution of the $i$th latent $z_i$ given observed $x_i$ and current
parameter estimate $\theta^{t-1}$.

Too bad you *still* can't evaluate this expectation. Employ a trick (in the discrete case)

$$
\begin{eqnarray} 
Q\( \theta, \theta^{t-1} \) &=&  \sum_{i=1}^N E\bigg[\log \big[ p\( z_i | \theta \) p\( x_i | z_i , \theta \) \big] \bigg| x_i, \theta^{t-1} \Bigg]    \nonumber \\
&=& \sum_{i=1}^N E\Bigg[\log \Bigg[ \prod_{k=1}^K \Bigg( p\( z_i = k | \theta \) p\( x_i | z_i = k , \theta \) \Bigg)^{I\(z_i=k\)} \Bigg] \Bigg| x_i, \theta^{t-1} \Bigg] \nonumber \\
&=&  \sum_{i=1}^N E\Bigg[ \sum_{k=1}^K \log \Bigg[ \Bigg( p\( z_i = k | \theta \) p\( x_i | z_i = k , \theta \) \Bigg)^{I\(z_i=k\)} \Bigg] \Bigg| x_i, \theta^{t-1} \Bigg]  \nonumber \\
&=&  \sum_{i=1}^N E\Bigg[ \sum_{k=1}^K I\(z_i=k\)\log \big[  p\( z_i =k | \theta \) p\( x_i | z_i =k , \theta \) \big] \Bigg| x_i, \theta^{t-1} \Bigg]  \nonumber \\
\end{eqnarray} 
$$

Now note that the only random variable in the final expression is $ I\(z_i=k\)$ because the probability factors in the log have $z_i = k$ fixed (the indicator "picked" the $k$). Therefore

$$
\begin{eqnarray} 
Q\( \theta, \theta^{t-1} \) &=&  \sum_{i=1}^N E\Bigg[ \sum_{k=1}^K I\(z_i=k\)\log \big[  p\( z_i =k | \theta \) p\( x_i | z_i =k , \theta \) \big] \Bigg| x_i, \theta^{t-1} \Bigg]  \nonumber \\
&=&  \sum_{i=1}^N  \sum_{k=1}^K E\big[ I\(z_i=k\) \big| x_i, \theta^{t-1}  \big]\log \big[  p\( z_i =k | \theta \) p\( x_i | z_i =k , \theta \) \big]  \label{eq:sample1}  \\
&=&  \sum_{i=1}^N  \sum_{k=1}^K p\(z_i=k \big| x_i, \theta^{t-1}\) \log \big[  p\( z_i =k | \theta \) p\( x_i | z_i =k , \theta \) \big]    \label{eq:sample2}

\end{eqnarray} 
$$

# The EM Algorithm

What's the EM algorithm?

1. Initialize parameters $\theta$ (somehow).

2. Given the parameters $\theta^{t-1}$ from the previous iteration of the algorithm, evaluate $Q$ so that it's only in terms of $\theta$. This is the expectation step and corresponds to the transition between 
lines \eqref{eq:sample1} and \eqref{eq:sample2} above.

3. Maximize $Q$ as a function of $\theta$. Go back to step 2 unless stopping criterion (e.g. $\| \theta^t -\theta^{t-1}\| < \delta$ for some $\delta$).
